---
title: TensorFlow Lite
tags:
  - ML
---



# TensorFlow Lite

TensorFlow Lite 是一组工具，可帮助开发者在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型。它支持设备端机器学习推断，延迟较低，并且二进制文件很小。

## **TensorFlow Lite 主要组件：**

- [TensorFlow Lite 解释器](https://tensorflow.google.cn/lite/guide/inference)，它可在手机、嵌入式 Linux 设备和微控制器等很多不同类型的硬件上运行经过专门优化的模型。
- [TensorFlow Lite 转换器](https://tensorflow.google.cn/lite/convert/index)，它可将 TensorFlow 模型转换为高效形式以供解释器使用，并可引入优化以减小二进制文件的大小和提高性能。

## **TensorFlow Lite 工作流：**

1. **选择模型**

   使用自己的 TensorFlow 模型，在线查找模型，或者[预训练模型](https://tensorflow.google.cn/lite/models)中选择一个模型直接使用或重新训练。

2. **转换模型**

   自定义模型使用 [TensorFlow Lite 转换器](https://tensorflow.google.cn/lite/convert/index)将模型转换为 TensorFlow Lite 格式，只需几行 Python 代码就能搞定。

   ```python
   import tensorflow as tf
   converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
   tflite_model = converter.convert()
   open("converted_model.tflite", "wb").write(tflite_model)
   ```

   [Netron](https://github.com/lutzroeder/netron)是可视化TensorFlow Lite模型的最简单方法。

3. **部署到设备**

   *推断*是通过模型运行数据以获得预测的过程。它需要一个模型，一个解释器和输入数据。

   使用 [TensorFlow Lite 解释器](https://tensorflow.google.cn/lite/guide/inference)（具有支持多种语言的 API）在设备端运行您的模型。

   解释器可在多个平台上工作，并提供了一个简单的API，用于运行Java，Swift，Objective-C，C ++和Python中的TensorFlow Lite模型。

   > 嵌入式Linux是用于部署机器学习的重要平台。要开始使用Python对TensorFlow Lite模型执行推理，请遵循[Python quickstart](https://tensorflow.google.cn/lite/guide/python)。
   >
   > 要改为安装C ++库，请参阅[Raspberry Pi](https://tensorflow.google.cn/lite/guide/build_rpi)或[基于Arm64的板](https://tensorflow.google.cn/lite/guide/build_arm64)的构建说明 （适用于Odroid C2，Pine64和NanoPi等板）。

4. **优化模型**

   TensorFlow Lite提供了一些工具 [模型优化工具包](https://tensorflow.google.cn/lite/guide/get_started#model_optimization_toolkit)来优化模型的大小和性能，而对精度的影响通常很小。优化的模型可能需要稍微更复杂的训练，转换或集成。

   模型优化的目标是在给定设备上达到性能，模型尺寸和精度的理想平衡。

   通过降低模型中值和运算的精度，**量化**可以减少模型的大小和推理所需的时间。

   ```python
   import tensorflow as tf
   converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
   converter.optimizations = [tf.lite.Optimize.DEFAULT] # 动态范围量化
   tflite_quantized_model = converter.convert()
   open("converted_model.tflite", "wb").write(tflite_quantized_model)
   ```

## 推断/推理

TensorFlow Lite解释器旨在实现精简和快速。解释器使用静态图排序和自定义（较少动态）内存分配器，以确保最小的负载，初始化和执行延迟。

1. **载入模型**

   将`.tflite`模型加载到内存中，该内存包含模型的执行图。

2. **转换数据**

   模型的原始输入数据通常与模型期望的输入数据格式不匹配。例如，您可能需要调整图像大小或更改图像格式以与模型兼容。

3. **运行推断**

   此步骤涉及使用TensorFlow Lite API执行模型。涉及几个步骤，例如构建解释器和分配张量。

4. **解释输出**

   当您从模型推断中收到结果时，必须以有意义的方式解释张量。

   例如，模型可能只返回概率列表。您可以将概率映射到相关类别并将其呈现给最终用户。

## [在Python中加载并运行模型](https://tensorflow.google.cn/lite/guide/inference#load_and_run_a_model_in_python)

从 [`tf.lite.Interpreter`](https://tensorflow.google.cn/api_docs/python/tf/lite/Interpreter) 加载模型并运行推理。

```python
# 使用Python解释器加载 .tflite文件并使用随机输入数据运行推理
import numpy as np
import tensorflow as tf

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path="converted_model.tflite")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```

要使用Python快速运行TensorFlow Lite模型，可以仅安装TensorFlow Lite解释程序，而不是安装所有TensorFlow软件包。

```python
import tflite_runtime.interpreter as tflite
interpreter = tflite.Interpreter(model_path=args.model_file)
```

## [在C ++中加载并运行模型](https://tensorflow.google.cn/lite/guide/inference#load_and_run_a_model_in_c)

在C ++中，模型存储在 [`FlatBufferModel`](https://tensorflow.google.cn/lite/api_docs/cc/class/tflite/flat-buffer-model.html) 类中。

```c++
// Load the model
std::unique_ptr<tflite::FlatBufferModel> model =
    tflite::FlatBufferModel::BuildFromFile(filename);

// Build the interpreter
tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptr<tflite::Interpreter> interpreter;
tflite::InterpreterBuilder(*model, resolver)(&interpreter);

// Resize input tensors, if desired.
interpreter->AllocateTensors();

float* input = interpreter->typed_input_tensor<float>(0);
// Fill `input`.

interpreter->Invoke();

float* output = interpreter->typed_output_tensor<float>(0);
```

## 模型优化

边缘设备通常具有有限的内存或计算能力。可以对模型应用各种优化，以便可以在这些约束条件下运行它们。另外，一些优化允许使用专用硬件来加速推理。

TensorFlow Lite和 [TensorFlow模型优化工具包](https://tensorflow.google.cn/model_optimization) 提供了可将优化推理的复杂性降至最低的工具。

[**开发流程**](https://tensorflow.google.cn/lite/performance/model_optimization#development_workflow)

**模型优化领域**涉及多种技术：

- 通过剪枝和结构化剪枝减少参数数量。
- 通过量化降低表示法精度。
- 将原始模型拓扑更新为更高效的拓扑，后者的参数数量更少或执行速度更快。例如，张量分解方法和蒸馏

**模型优化可以通过几种主要方法来帮助应用程序开发。**

1. 缩小尺寸 size_reduction ：量化都可以减小模型的大小，这可能会牺牲一些准确性。修剪和聚类可以通过使其更易于压缩来减小下载模型的大小。

2. 减小延迟 latency_reduction : *延迟*是对给定模型运行单个推理所花费的时间。可以通过简化推断过程中发生的计算来使用量化来减少等待时间，这可能会牺牲一些准确性。

3. 加速器兼容 accelerator_compatibility ：某些硬件加速器（例如 [Edge TPU](https://cloud.google.com/edge-tpu/)）可以使用经过正确优化的模型以极快的速度运行推理。通常，这些类型的设备要求以特定方式量化模型。

4. 权衡取舍 trade-offs ：TensorFlow Lite当前支持通过量化，修剪和聚类进行优化。

   这些是[TensorFlow模型优化工具包的](https://tensorflow.google.cn/model_optimization)一部分，该工具包提供了与TensorFlow Lite兼容的模型优化技术的资源。

### [量化](https://tensorflow.google.cn/lite/performance/model_optimization#quantization)

通过降低用于表示模型参数的数字的精度来工作，这些参数默认为32位浮点数。这样可以减小模型尺寸并加快计算速度。

**训练后量化 Post-training quantization** 训练后量化包括减少CPU和硬件加速器的等待时间，处理，功耗和模型大小而模型精度几乎不降低的常规技术。这些技术可以在已经训练好的浮动TensorFlow模型上执行，并在TensorFlow Lite转换期间应用。

- [训练后动态范围量化](https://tensorflow.google.cn/lite/performance/post_training_quant)
- [训练后全整数量化](https://tensorflow.google.cn/lite/performance/post_training_integer_quant)
- [训练后float16量化](https://tensorflow.google.cn/lite/performance/post_training_float16_quant)

### 剪枝

[修剪](https://tensorflow.google.cn/model_optimization/guide/pruning)工作原理是删除模型中对其预测影响很小的参数。修剪后的模型在磁盘上的大小相同，并且具有相同的运行时延迟，但是可以更有效地进行压缩。这使剪枝成为减少模型下载大小的有用技术。

### 聚类

[聚类](https://tensorflow.google.cn/model_optimization/guide/clustering) 工作方式是将模型中每个图层的权重分组为预定义数量的簇，然后共享属于每个单独簇的权重的质心值。这减少了模型中唯一权重值的数量，从而降低了其复杂性。

结果，可以更有效地压缩集群模型，从而提供类似于修剪的部署优势。