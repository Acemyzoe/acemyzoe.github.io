---
title: TensorFlow Lite
tags:
  - ML
---



# TensorFlow Lite

TensorFlow Lite 是一组工具，可帮助开发者在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型。它支持设备端机器学习推断，延迟较低，并且二进制文件很小。

## **TensorFlow Lite 主要组件：**

- [TensorFlow Lite 解释器](https://tensorflow.google.cn/lite/guide/inference)，它可在手机、嵌入式 Linux 设备和微控制器等很多不同类型的硬件上运行经过专门优化的模型。
- [TensorFlow Lite 转换器](https://tensorflow.google.cn/lite/convert/index)，它可将 TensorFlow 模型转换为高效形式以供解释器使用，并可引入优化以减小二进制文件的大小和提高性能。

## **TensorFlow Lite 工作流：**

1. **选择模型**

   使用自己的 TensorFlow 模型，在线查找模型，或者[预训练模型](https://tensorflow.google.cn/lite/models)中选择一个模型直接使用或重新训练。

2. **转换模型**

   自定义模型使用 [TensorFlow Lite 转换器](https://tensorflow.google.cn/lite/convert/index)将模型转换为 TensorFlow Lite 格式，只需几行 Python 代码就能搞定。

   ```python
   import tensorflow as tf
   converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
   tflite_model = converter.convert()
   open("converted_model.tflite", "wb").write(tflite_model)
   ```

   [Netron](https://github.com/lutzroeder/netron)是可视化TensorFlow Lite模型的最简单方法。

3. **部署到设备**

   *推断*是通过模型运行数据以获得预测的过程。它需要一个模型，一个解释器和输入数据。

   使用 [TensorFlow Lite 解释器](https://tensorflow.google.cn/lite/guide/inference)（具有支持多种语言的 API）在设备端运行您的模型。

   解释器可在多个平台上工作，并提供了一个简单的API，用于运行Java，Swift，Objective-C，C ++和Python中的TensorFlow Lite模型。

   > 嵌入式Linux是用于部署机器学习的重要平台。要开始使用Python对TensorFlow Lite模型执行推理，请遵循[Python quickstart](https://tensorflow.google.cn/lite/guide/python)。
   >
   > 要改为安装C ++库，请参阅[Raspberry Pi](https://tensorflow.google.cn/lite/guide/build_rpi)或[基于Arm64的板](https://tensorflow.google.cn/lite/guide/build_arm64)的构建说明 （适用于Odroid C2，Pine64和NanoPi等板）。

4. **优化模型**

   TensorFlow Lite提供了一些工具 [模型优化工具包](https://tensorflow.google.cn/lite/guide/get_started#model_optimization_toolkit)来优化模型的大小和性能，而对精度的影响通常很小。优化的模型可能需要稍微更复杂的训练，转换或集成。

   模型优化的目标是在给定设备上达到性能，模型尺寸和精度的理想平衡。

   通过降低模型中值和运算的精度，**量化**可以减少模型的大小和推理所需的时间。

   ```python
   import tensorflow as tf
   converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
   converter.optimizations = [tf.lite.Optimize.DEFAULT]
   tflite_quantized_model = converter.convert()
   open("converted_model.tflite", "wb").write(tflite_quantized_model)
   ```

## 推断/推理

TensorFlow Lite解释器旨在实现精简和快速。解释器使用静态图排序和自定义（较少动态）内存分配器，以确保最小的负载，初始化和执行延迟。

1. **载入模型**

   将`.tflite`模型加载到内存中，该内存包含模型的执行图。

2. **转换数据**

   模型的原始输入数据通常与模型期望的输入数据格式不匹配。例如，您可能需要调整图像大小或更改图像格式以与模型兼容。

3. **运行推断**

   此步骤涉及使用TensorFlow Lite API执行模型。涉及几个步骤，例如构建解释器和分配张量。

4. **解释输出**

   当您从模型推断中收到结果时，必须以有意义的方式解释张量。

   例如，模型可能只返回概率列表。您可以将概率映射到相关类别并将其呈现给最终用户。

## [在Python中加载并运行模型](https://tensorflow.google.cn/lite/guide/inference#load_and_run_a_model_in_python)

从 [`tf.lite.Interpreter`](https://tensorflow.google.cn/api_docs/python/tf/lite/Interpreter) 加载模型并运行推理。

```python
# 使用Python解释器加载 .tflite文件并使用随机输入数据运行推理
import numpy as np
import tensorflow as tf

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path="converted_model.tflite")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```

要使用Python快速运行TensorFlow Lite模型，可以仅安装TensorFlow Lite解释程序，而不是安装所有TensorFlow软件包。

```python
import tflite_runtime.interpreter as tflite
interpreter = tflite.Interpreter(model_path=args.model_file)
```

## [在C ++中加载并运行模型](https://tensorflow.google.cn/lite/guide/inference#load_and_run_a_model_in_c)

在C ++中，模型存储在 [`FlatBufferModel`](https://tensorflow.google.cn/lite/api_docs/cc/class/tflite/flat-buffer-model.html) 类中。

```c++
// Load the model
std::unique_ptr<tflite::FlatBufferModel> model =
    tflite::FlatBufferModel::BuildFromFile(filename);

// Build the interpreter
tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptr<tflite::Interpreter> interpreter;
tflite::InterpreterBuilder(*model, resolver)(&interpreter);

// Resize input tensors, if desired.
interpreter->AllocateTensors();

float* input = interpreter->typed_input_tensor<float>(0);
// Fill `input`.

interpreter->Invoke();

float* output = interpreter->typed_output_tensor<float>(0);
```

